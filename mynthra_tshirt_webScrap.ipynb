{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Myntra T-Shirt Review Data Harvesting & Product Details Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for web scraping and data manipulation\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to keep track of file names\n",
    "file_name_counter = 0\n",
    "\n",
    "# Function for scraping Myntra data\n",
    "def myntra_data_scrap(link):\n",
    "\n",
    "    global file_name_counter\n",
    "    file_name_counter += 1\n",
    "    \n",
    "    # Create a new instance of the Edge driver\n",
    "    driver = webdriver.Edge()\n",
    "\n",
    "    # Dictionary to store product information\n",
    "    Product_Information = {'Product_link' :[],\n",
    "                            'Product_id': [],\n",
    "                            'Product_Name':[],\n",
    "                            'Product_Description':[],\n",
    "                            'Discounted_Price':[],\n",
    "                            'Original_Price':[],\n",
    "                            'Discount_Percentage':[],\n",
    "                            'Fabric_Material':[],\n",
    "                            'Neck_Type':[],\n",
    "                            'overall_rating':[],\n",
    "                            'votes':[],\n",
    "                            'Customer_Rated_Rating':[],\n",
    "                            'Customer_Review_Text':[],\n",
    "                            'Reviewers_Username':[],\n",
    "                            'Date_of_Review':[],\n",
    "                            'Reviewers_Product_Images':[]\n",
    "                            }\n",
    "\n",
    "    # Loop through the provided links\n",
    "    for i in link:\n",
    "        driver.get(i)\n",
    "\n",
    "        # Wait for 3 seconds to let the page load\n",
    "        time.sleep(3)\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "               \n",
    "        # get the product name\n",
    "        try:\n",
    "            brand = soup.find('h1', attrs= {'class': 'pdp-title'}).text\n",
    "            \n",
    "        except:\n",
    "            brand = np.NaN \n",
    "        \n",
    "        # Get the description    \n",
    "        try:\n",
    "            description = soup.find('h1', attrs= {'class': 'pdp-name'}).text\n",
    "            \n",
    "        except:\n",
    "            description = np.NaN \n",
    "\n",
    "        # Get the overall-rating and votes\n",
    "        try:\n",
    "            rating = soup.find('div', attrs={'class': 'index-overallRating'}).text\n",
    "            overall_rating = rating[:3]\n",
    "            votes = rating[4:].rstrip('Ratings ')\n",
    "        except:\n",
    "            overall_rating = np.NaN\n",
    "            votes = np.NaN\n",
    "\n",
    "        # Get the discount price\n",
    "        try:\n",
    "            discount_price = soup.find('span', attrs= {'class': 'pdp-price'}).text\n",
    "            \n",
    "        except:\n",
    "            discount_price = np.NaN \n",
    "\n",
    "        # Get the actual price    \n",
    "        try:\n",
    "            actual_price = soup.find('span', attrs= {'class': 'pdp-mrp'}).text\n",
    "            \n",
    "        except:\n",
    "            actual_price = np.NaN \n",
    "\n",
    "        # Get the discount percentage    \n",
    "        try:\n",
    "            discount_percentage = soup.find('span', attrs= {'class': 'pdp-discount'}).text\n",
    "            \n",
    "        except:\n",
    "            discount_percentage = np.NaN \n",
    "        \n",
    "        # Get the product specification \n",
    "        try:\n",
    "            spec = soup.find('div', {'class': 'index-tableContainer'})\n",
    "\n",
    "            specification = {}\n",
    "            for row in spec:\n",
    "                key = row.find('div', attrs={'class':'index-rowKey'}).text.strip()\n",
    "                value = row.find('div', attrs={'class':'index-rowValue'}).text.strip()\n",
    "                specification[key] = value\n",
    "            if specification:\n",
    "                fabric = specification['Fabric']\n",
    "                neck = specification['Neck']\n",
    "            else: \n",
    "                fabric = np.NaN\n",
    "                neck = np.NaN\n",
    "        except:\n",
    "            specification = {'Fabric': np.NaN,\n",
    "                            'Neck': np.NaN\n",
    "                            }\n",
    "\n",
    "        \n",
    "        id = soup.find('span', attrs={'class':\"supplier-styleId\"}).text\n",
    "        driver.get('https://www.myntra.com/reviews/'+id)\n",
    "        \n",
    "\n",
    "        # Wait for the page to load\n",
    "        wait = WebDriverWait(driver, 3)\n",
    "        try:    \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.user-review-reviewTextWrapper')))\n",
    "\n",
    "\n",
    "            customer_review_image = []\n",
    "\n",
    "            try:\n",
    "                \n",
    "                # Get the reviewer images\n",
    "                customer_image = driver.find_elements(By.CSS_SELECTOR, '.image-thumb-wrapper-image')\n",
    "                for image in customer_image:\n",
    "                    photo = image.get_attribute('src')\n",
    "                    if photo:\n",
    "                        customer_review_image.append(photo)\n",
    "            except: \n",
    "                customer_review_image = np.NaN\n",
    "\n",
    "\n",
    "            # Get the initial reviews\n",
    "            review_text = []\n",
    "            customer_rating = []\n",
    "            reviewer_name = []\n",
    "            review_date = []\n",
    "                \n",
    "            # Scroll down to load more reviews\n",
    "            body = driver.find_element(By.TAG_NAME, 'body')\n",
    "            for _ in range(35):  # Adjust the range based on the number of scrolls you want\n",
    "                body.send_keys(Keys.PAGE_DOWN)\n",
    "                time.sleep(1)\n",
    "\n",
    "            # Wait for the new reviews to load\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.user-review-reviewTextWrapper')))\n",
    "\n",
    "            try:    \n",
    "                # Get the reviewer name and date\n",
    "                reviews = driver.find_elements(By.CSS_SELECTOR, '.user-review-reviewTextWrapper')\n",
    "                for review in reviews:\n",
    "                    review_text.append(review.text.replace('\\n', ''))\n",
    "            except:\n",
    "                review_text = np.NaN\n",
    "\n",
    "            try:\n",
    "                rating = driver.find_elements(By.CSS_SELECTOR, '.user-review-starWrapper')\n",
    "                #for rate in rating:\n",
    "                for rate in rating:\n",
    "                    spans = rate.find_elements(By.TAG_NAME, 'span')\n",
    "                    if spans:\n",
    "                        reviewer_rate = spans[0].text\n",
    "                        customer_rating.append(reviewer_rate)\n",
    "            except:\n",
    "                customer_rating = np.NaN\n",
    "\n",
    "            try:\n",
    "                comment_name = driver.find_elements(By.CSS_SELECTOR, '.user-review-left')\n",
    "                for name in comment_name:\n",
    "                    # Extracting text content of each span within .user-review-left\n",
    "                    spans = name.find_elements(By.TAG_NAME, 'span')\n",
    "                    if spans:\n",
    "                        commenter_name = spans[0].text\n",
    "                        review_dates = spans[1].text\n",
    "                        reviewer_name.append(commenter_name)\n",
    "                        review_date.append(review_dates)\n",
    "            except:\n",
    "                review_date = np.NaN\n",
    "                reviewer_name = np.NaN\n",
    "                \n",
    "        except:\n",
    "            customer_review_image = np.NaN\n",
    "            review_text = np.NaN\n",
    "            customer_rating = np.NaN\n",
    "            reviewer_name = np.NaN\n",
    "            review_date = np.NaN\n",
    "            \n",
    "  \n",
    "        Product_Information['Product_link'].append(i)\n",
    "        Product_Information['Product_id'].append(id)\n",
    "        Product_Information['Product_Name'].append(brand)\n",
    "        Product_Information['Product_Description'].append(description)\n",
    "        Product_Information['Discounted_Price'].append(discount_price)\n",
    "        Product_Information['Original_Price'].append(actual_price)\n",
    "        Product_Information['Discount_Percentage'].append(discount_percentage)\n",
    "        Product_Information['Fabric_Material'].append(fabric)\n",
    "        Product_Information['Neck_Type'].append(neck)\n",
    "        Product_Information['overall_rating'].append(overall_rating)\n",
    "        Product_Information['votes'].append(votes)\n",
    "        Product_Information['Customer_Rated_Rating'].append(customer_rating)\n",
    "        Product_Information['Customer_Review_Text'].append(review_text)\n",
    "        Product_Information['Reviewers_Username'].append(reviewer_name)\n",
    "        Product_Information['Date_of_Review'].append(review_date)\n",
    "        Product_Information['Reviewers_Product_Images'].append(customer_review_image)\n",
    "\n",
    "    df = pd.DataFrame(Product_Information)\n",
    "        \n",
    "    file_name = f'mynthra_tshirt_data_{file_name_counter}.csv'\n",
    "\n",
    "    df.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    return print(file_name,'Completed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Harvesting with user defined Batch size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(link_list, batch_size, current_batch=1):\n",
    "    # Calculate the start and end indices for the current batch\n",
    "    start_index = (current_batch - 1) * batch_size\n",
    "    end_index = current_batch * batch_size\n",
    "\n",
    "    # Extract the links for the current batch\n",
    "    current_batch_links = link_list[start_index:end_index]\n",
    "    myntra_data_scrap(current_batch_links)\n",
    "\n",
    "    # If there are more links, recursively call the function for the next batch\n",
    "    if end_index < len(link_list):\n",
    "        create_batches(link_list, batch_size, current_batch + 1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset with weblinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 121197 entries, 0 to 121196\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   links   121197 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 947.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_link = pd.read_csv('mynthra_tshirt_link_full.csv')\n",
    "df_link.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list from the dataframe\n",
    "tshirt_link = df_link['links'].tolist()\n",
    "\n",
    "# Mention the  batch size \n",
    "batch_size = 1000\n",
    "create_batches(tshirt_link_list, batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
